#!/bin/bash -ex

repo_path="$(git rev-parse --show-toplevel)"

platform="unknown"
if [[ "$(uname)" == "Linux" ]]; then
  platform="linux"
elif [[ "$(uname)" == "Darwin" ]]; then
  platform="mac"
fi

bucket_name="test-connect-$(uuidgen | tr \"[:upper:]\" \"[:lower:]\")"

check_pod_is_ready() {
  local app_name="$1"
  local namespace="${2:-default}"
  local jsonpath='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'
  local pod_conditions="$(kubectl get po -n "$namespace" -l app="$app_name" -o jsonpath="$jsonpath" 2>&1)"
  echo "$pod_conditions" | grep "Ready=True"
}

check_jobs_are_done() {
  local job_name="$(kubectl get jobs --sort-by='{.metadata.creationTimestamp}' | tail -1 | grep s3-connector-job | awk '{ print $1 }')"
  local jsonpath="{@.status}"
  local job_conditions="$(kubectl get jobs $job_name -n default -o jsonpath=$jsonpath)"
  echo "$job_conditions" | grep "type:Complete" | grep "status:True"
}

install_aws_cli() {
  curl "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip" && unzip awscli-bundle.zip && sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
}

prep_s3() {
  command -v aws &>/dev/null || install_aws_cli
  aws s3 mb "s3://$bucket_name" --region "us-west-2"
}

check_s3() {
  local topic="$1"
  local year="$2"
  local month="$3"
  local day="$4"
  local partition="topics/$topic/year=$year/month=$month/day=$day"
  aws s3 ls "s3://$bucket_name/$partition/"
}

check_s3_user_event() {
  local topic="$1"
  local product_instance_id="$2"
  local product_id="$3"
  local user_oid="$4"
  local year="$5"
  local month="$6"
  local day="$7"
  local partition="topics/$topic/product_instance_id=$product_instance_id/product_id=$product_id/user_oid=$user_oid/year=$year/month=$month/day=$day"
  aws s3 ls "s3://$bucket_name/$partition/"
}

check_s3_sites() {
  local topic="$1"
  local customer_id="$2"
  local product_id="$3"
  local instance_id="$4"
  local year="$5"
  local month="$6"
  local day="$7"
  local partition="topics/$topic/customer_id=$customer_id/product_id=$product_id/instance_id=$instance_id/year=$year/month=$month/day=$day"
  aws s3 ls "s3://$bucket_name/$partition/"
}

check_s3_health_metrics() {
  local topic="$1"
  local product_instance_id="$2"
  local product_id="$3"
  local year="$4"
  local month="$5"
  local day="$6"
  local partition="topics/$topic/product_instance_id=$product_instance_id/product_id=$product_id/year=$year/month=$month/day=$day"
  aws s3 ls "s3://$bucket_name/$partition/"
}

clean_up_s3() {
  aws s3 rb "s3://$bucket_name" --force
}

repeat_and_sleep() {
  local max_sleep="${3:-30}"
  total_sleep=0
  local command_result="$($1)"
  until [[ -n "$command_result" || "$total_sleep" -ge "$max_sleep" ]]; do
    sleep "$2"
    local command_result="$($1)"
    let total_sleep+="$2"
  done
  if [[ "$total_sleep" -ge "$max_sleep" ]]; then
      exit 1
  fi
}

check_for_secret() {
  local secret_name="$1"
  kubectl get secrets | grep -q "$secret_name"
}

set_quay_secret() {
  #NOTE: requires QUAY_USERNAME and QUAY_PASSWORD to be set in environment
  local namespace="${1:-default}"
  local base64_cmd="base64"
  if [ "$platform" = "linux" ]; then
    local base64_cmd="base64 -w 0"
  fi
  set +x
  local quay_auth="$(echo -n "$QUAY_USERNAME:$QUAY_PASSWORD" | $base64_cmd)"
  local quay_auth_json="$(echo -n '{
  "auths": {
    "quay.io": {
      "auth": "'"$quay_auth"'",
      "email": ""
    }
  }
}' | $base64_cmd)"

  printf "apiVersion: v1\nkind: Secret\nmetadata:\n  name: quay-sts\ndata:\n  .dockerconfigjson: %s\ntype: kubernetes.io/dockerconfigjson" "$quay_auth_json" > quay-sts.yaml
  set -x
  kubectl create -f quay-sts.yaml -n "$namespace"
  rm quay-sts.yaml
}

set_aws_secret() {
  set +x
  local namespace="${1:-default}"
  if [ ! -d "$HOME/.aws" ]; then
    mkdir -p "$HOME/.aws"
  fi
  if [ ! -f "$HOME/.aws/credentials" ]; then
    echo -n '[default]
aws_access_key_id = '"$AWS_ACCESS_KEY_ID"'
aws_secret_access_key = '"$AWS_SECRET_ACCESS_KEY" > "$HOME/.aws/credentials"
  fi
  set -x
  kubectl create secret generic aws-s3-creds --from-file="$HOME/.aws/credentials" -n "$namespace"
}


exit_script() {
  exit_code="$?"
  #NOTE: comment the next line to not clean up s3 buckets during test
  clean_up_s3
  set +x
  if [ "$exit_code" = 0 ]; then
      echo "!!!!!!!   TEST PASSED    !!!!!!!"
  else
      echo "!!!!!!!   TEST FAILED    !!!!!!!"
  fi
  set -x
  exit "$exit_code"
}

main () {
  local git_sha="$(git rev-parse HEAD)"
  trap exit_script SIGHUP SIGINT SIGKILL SIGSTOP SIGTERM EXIT
  check_for_secret "aws-s3-creds" || set_aws_secret
  check_for_secret "quay-sts" || set_quay_secret
  prep_s3
  cd prism-lts
  helm dep update
  helm dep build
  helm upgrade --install test-prism-lts . \
       --set kafkaConnect.tag="$git_sha" \
       --set s3ConnectorJob.tag="$git_sha" \
       --set s3ConnectorJob.config.S3_BUCKET_NAME="$bucket_name" \
       --set tags.prism-lts-test-values=true \
       --wait --timeout 600 --debug

  repeat_and_sleep 'check_pod_is_ready prism-lts' "1"
  repeat_and_sleep 'check_pod_is_ready local-kafka-rest' "1"
  repeat_and_sleep 'check_jobs_are_done' "15" "120"

  # Send the Data
  /bin/bash "$repo_path/bin/send_a_bunch_of_data" "ac-user-event"
  /bin/bash "$repo_path/bin/send_a_bunch_of_data" "site-modules"
  /bin/bash "$repo_path/bin/send_a_bunch_of_data" "site-usage-metrics"
  /bin/bash "$repo_path/bin/send_a_bunch_of_data" "health-metrics"
  /bin/bash "$repo_path/bin/send_a_bunch_of_data" "saas-usage-metrics"

  # AC User Event
  repeat_and_sleep 'check_s3_user_event ac-user-event thebest wakawakawaka someothergreatinfo 2018 01 04' "2" "80"

  # Site Modules
  repeat_and_sleep 'check_s3_sites site-modules acme ALM asd 2018 01 04' "2" "80"

  # Site Usage Metrics
  repeat_and_sleep 'check_s3_sites site-usage-metrics acme ALM asd 2018 01 04' "2" "80"

  # Health Metrics
  repeat_and_sleep 'check_s3_health_metrics health-metrics 125524 ALM 2018 01 04' "2" "80"

  # Saas Usage Metrics Value
  repeat_and_sleep 'check_s3_health_metrics saas-usage-metrics sassy MLA 2018 01 04' "2" "80"

}

main "$@"